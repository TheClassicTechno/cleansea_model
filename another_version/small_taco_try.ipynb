{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "small taco try",
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "authorship_tag": "ABX9TyMKfPlpLyj+xw+7WIX8TMlW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheClassicTechno/cleansea_model/blob/main/another_version/small_taco_try.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6i0R_D6Gk89",
        "outputId": "9dfd99a6-891f-4595-8814-2c0d2934d396"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmfB3Q5xGXAy",
        "outputId": "2c975a2c-747e-426b-bcdd-8d09f80ff8e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 163 files belonging to 4 classes.\n",
            "Using 131 files for training.\n",
            "Found 163 files belonging to 4 classes.\n",
            "Using 32 files for validation.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rescaling (Rescaling)       (None, 256, 256, 3)       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 256, 256, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 128, 128, 16)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128, 128, 16)      0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 128, 128, 32)      4640      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 64, 64, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64, 64, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 64, 64, 16)        4624      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 32, 32, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 32, 32, 16)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 16384)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                1048640   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 260       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,058,612\n",
            "Trainable params: 1,058,612\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "import cv2\n",
        "import glob\n",
        "\n",
        "#processing images\n",
        "bottles1 = '/content/drive/MyDrive/TEAM9SURESTART/taco/bottles'\n",
        "cans1 = '/content/drive/MyDrive/TEAM9SURESTART/taco/cans'\n",
        "containers1 = '/content/drive/MyDrive/TEAM9SURESTART/taco/containers'\n",
        "#paper1 = '/content/drive/MyDrive/TEAM9SURESTART/taco/paper'\n",
        "plastic1 = '/content/drive/MyDrive/TEAM9SURESTART/taco/plastic'\n",
        "\n",
        "#all images are 227x227 in RGB so 227, 227, 3\n",
        "bottles = [cv2.imread(image) for image in glob.glob(bottles1)]\n",
        "cans = [cv2.imread(image) for image in glob.glob(cans1)]\n",
        "containers = [cv2.imread(image) for image in glob.glob(containers1)]\n",
        "#paper = [cv2.imread(image) for image in glob.glob(paper1)]\n",
        "plastic = [cv2.imread(image) for image in glob.glob(plastic1)]\n",
        "\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    '/content/drive/MyDrive/TEAM9SURESTART/taco',\n",
        "    validation_split = 0.2,\n",
        "    subset = \"training\",\n",
        "    seed = 24,  \n",
        "    image_size = (256, 256),\n",
        "    batch_size = 32\n",
        ")\n",
        "\n",
        "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    '/content/drive/MyDrive/TEAM9SURESTART/taco',\n",
        "    validation_split = 0.2,\n",
        "    subset = \"validation\",\n",
        "    seed = 24,  \n",
        "    image_size = (256, 256),\n",
        "    batch_size = 32\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "#X_train, X_test, y_train, y_test = train_test_split(train_dataset, val_dataset, test_size = 0.1, random_state = 1, stratify = val_dataset)\n",
        "\n",
        "from scipy.stats import zscore\n",
        "import pandas as pd\n",
        "\n",
        "# Create a sample df\n",
        "\n",
        "# Calculate the zscores and drop zscores into new column\n",
        "\n",
        "train_dataset = train_dataset.cache().shuffle(13).prefetch(buffer_size = tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.cache().shuffle(13).prefetch(buffer_size = tf.data.AUTOTUNE)\n",
        "\n",
        "model = Sequential ([\n",
        "    layers.Rescaling(1./255, input_shape = (256, 256, 3)),\n",
        "    layers.Conv2D(16, 3, padding = 'same', activation = 'relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Conv2D(32, 3, padding = 'same', activation = 'relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Conv2D(16, 3, padding = 'same', activation = 'relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation = 'relu'),\n",
        "    layers.Dense(4, activation = 'softmax')\n",
        "])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sE1BaEjvAfo_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), metrics = ['accuracy'])\n",
        "'''\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    metrics=['accuracy'],  run_eagerly=True\n",
        ")\n",
        "'''\n",
        "\n",
        "model.fit (\n",
        "    train_dataset, \n",
        "    validation_data = val_dataset,\n",
        "    epochs = 250\n",
        ")\n",
        "#model.save('model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4CaGQFS2Hlk6",
        "outputId": "f22e087d-43e5-4c16-d916-1ec559a38770"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 17s 299ms/step - loss: 2.7161 - accuracy: 0.2519 - val_loss: 1.3580 - val_accuracy: 0.2812\n",
            "Epoch 2/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 1.4398 - accuracy: 0.3664 - val_loss: 1.3706 - val_accuracy: 0.5000\n",
            "Epoch 3/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3710 - accuracy: 0.3130 - val_loss: 1.3836 - val_accuracy: 0.3750\n",
            "Epoch 4/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3786 - accuracy: 0.3130 - val_loss: 1.3859 - val_accuracy: 0.0938\n",
            "Epoch 5/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 1.3801 - accuracy: 0.3282 - val_loss: 1.3853 - val_accuracy: 0.2812\n",
            "Epoch 6/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3773 - accuracy: 0.3511 - val_loss: 1.3841 - val_accuracy: 0.2812\n",
            "Epoch 7/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 1.3753 - accuracy: 0.4046 - val_loss: 1.3824 - val_accuracy: 0.2812\n",
            "Epoch 8/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3717 - accuracy: 0.3969 - val_loss: 1.3789 - val_accuracy: 0.2812\n",
            "Epoch 9/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 1.3654 - accuracy: 0.3588 - val_loss: 1.3743 - val_accuracy: 0.2812\n",
            "Epoch 10/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3602 - accuracy: 0.3359 - val_loss: 1.3673 - val_accuracy: 0.2812\n",
            "Epoch 11/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3643 - accuracy: 0.3206 - val_loss: 1.3578 - val_accuracy: 0.2812\n",
            "Epoch 12/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3674 - accuracy: 0.3206 - val_loss: 1.3584 - val_accuracy: 0.2812\n",
            "Epoch 13/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3629 - accuracy: 0.3206 - val_loss: 1.3645 - val_accuracy: 0.2812\n",
            "Epoch 14/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3514 - accuracy: 0.3206 - val_loss: 1.3675 - val_accuracy: 0.2812\n",
            "Epoch 15/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3444 - accuracy: 0.3206 - val_loss: 1.3647 - val_accuracy: 0.2812\n",
            "Epoch 16/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3523 - accuracy: 0.3206 - val_loss: 1.3629 - val_accuracy: 0.2812\n",
            "Epoch 17/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3473 - accuracy: 0.3206 - val_loss: 1.3588 - val_accuracy: 0.2812\n",
            "Epoch 18/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3422 - accuracy: 0.3206 - val_loss: 1.3588 - val_accuracy: 0.2812\n",
            "Epoch 19/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3440 - accuracy: 0.3206 - val_loss: 1.3537 - val_accuracy: 0.2812\n",
            "Epoch 20/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 1.3331 - accuracy: 0.3130 - val_loss: 1.3451 - val_accuracy: 0.2812\n",
            "Epoch 21/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3222 - accuracy: 0.3282 - val_loss: 1.3374 - val_accuracy: 0.3125\n",
            "Epoch 22/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3095 - accuracy: 0.3664 - val_loss: 1.3260 - val_accuracy: 0.3125\n",
            "Epoch 23/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.3090 - accuracy: 0.3588 - val_loss: 1.3388 - val_accuracy: 0.3125\n",
            "Epoch 24/250\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 1.2975 - accuracy: 0.3969 - val_loss: 1.2778 - val_accuracy: 0.4688\n",
            "Epoch 25/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 1.2761 - accuracy: 0.4198 - val_loss: 1.3572 - val_accuracy: 0.3125\n",
            "Epoch 26/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.2865 - accuracy: 0.3969 - val_loss: 1.3239 - val_accuracy: 0.3125\n",
            "Epoch 27/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 1.2523 - accuracy: 0.4580 - val_loss: 1.2673 - val_accuracy: 0.5000\n",
            "Epoch 28/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.2341 - accuracy: 0.4580 - val_loss: 1.3353 - val_accuracy: 0.3125\n",
            "Epoch 29/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.2511 - accuracy: 0.4351 - val_loss: 1.2481 - val_accuracy: 0.4375\n",
            "Epoch 30/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 1.1897 - accuracy: 0.4733 - val_loss: 1.3211 - val_accuracy: 0.4688\n",
            "Epoch 31/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 1.1618 - accuracy: 0.5115 - val_loss: 1.2452 - val_accuracy: 0.4375\n",
            "Epoch 32/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.1123 - accuracy: 0.5420 - val_loss: 1.2712 - val_accuracy: 0.5000\n",
            "Epoch 33/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 1.1201 - accuracy: 0.5191 - val_loss: 1.1960 - val_accuracy: 0.5000\n",
            "Epoch 34/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 1.0482 - accuracy: 0.5573 - val_loss: 1.3116 - val_accuracy: 0.4375\n",
            "Epoch 35/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 1.1515 - accuracy: 0.4580 - val_loss: 1.2011 - val_accuracy: 0.4375\n",
            "Epoch 36/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.1231 - accuracy: 0.4733 - val_loss: 1.2085 - val_accuracy: 0.4375\n",
            "Epoch 37/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 1.0471 - accuracy: 0.5420 - val_loss: 1.2992 - val_accuracy: 0.3750\n",
            "Epoch 38/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.9915 - accuracy: 0.5496 - val_loss: 1.1231 - val_accuracy: 0.4375\n",
            "Epoch 39/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.9748 - accuracy: 0.6107 - val_loss: 1.2469 - val_accuracy: 0.5000\n",
            "Epoch 40/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.9100 - accuracy: 0.5725 - val_loss: 1.0577 - val_accuracy: 0.5000\n",
            "Epoch 41/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.8214 - accuracy: 0.6794 - val_loss: 1.2298 - val_accuracy: 0.5000\n",
            "Epoch 42/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.8173 - accuracy: 0.6489 - val_loss: 1.0347 - val_accuracy: 0.5938\n",
            "Epoch 43/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.7655 - accuracy: 0.7328 - val_loss: 1.2406 - val_accuracy: 0.5312\n",
            "Epoch 44/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.7291 - accuracy: 0.6870 - val_loss: 1.0773 - val_accuracy: 0.5938\n",
            "Epoch 45/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.6269 - accuracy: 0.7634 - val_loss: 1.0536 - val_accuracy: 0.6875\n",
            "Epoch 46/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.5279 - accuracy: 0.7939 - val_loss: 1.1496 - val_accuracy: 0.5625\n",
            "Epoch 47/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.5487 - accuracy: 0.8321 - val_loss: 1.0226 - val_accuracy: 0.7188\n",
            "Epoch 48/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.6484 - accuracy: 0.7710 - val_loss: 1.2453 - val_accuracy: 0.5625\n",
            "Epoch 49/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.6217 - accuracy: 0.8015 - val_loss: 1.0151 - val_accuracy: 0.5312\n",
            "Epoch 50/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.5128 - accuracy: 0.8626 - val_loss: 1.1230 - val_accuracy: 0.5938\n",
            "Epoch 51/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.5458 - accuracy: 0.8321 - val_loss: 1.0303 - val_accuracy: 0.6250\n",
            "Epoch 52/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.4245 - accuracy: 0.8397 - val_loss: 1.0100 - val_accuracy: 0.7188\n",
            "Epoch 53/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.3016 - accuracy: 0.9313 - val_loss: 1.0758 - val_accuracy: 0.6562\n",
            "Epoch 54/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.2504 - accuracy: 0.9389 - val_loss: 1.1314 - val_accuracy: 0.6875\n",
            "Epoch 55/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.2489 - accuracy: 0.9160 - val_loss: 1.0971 - val_accuracy: 0.6875\n",
            "Epoch 56/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.2085 - accuracy: 0.9389 - val_loss: 1.3715 - val_accuracy: 0.5938\n",
            "Epoch 57/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.2138 - accuracy: 0.9466 - val_loss: 1.0286 - val_accuracy: 0.7188\n",
            "Epoch 58/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.2901 - accuracy: 0.8702 - val_loss: 1.1101 - val_accuracy: 0.7188\n",
            "Epoch 59/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.2119 - accuracy: 0.9466 - val_loss: 1.1438 - val_accuracy: 0.6562\n",
            "Epoch 60/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.2671 - accuracy: 0.9466 - val_loss: 0.9716 - val_accuracy: 0.7188\n",
            "Epoch 61/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.2534 - accuracy: 0.9466 - val_loss: 1.0060 - val_accuracy: 0.7188\n",
            "Epoch 62/250\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.1929 - accuracy: 0.9389 - val_loss: 1.0438 - val_accuracy: 0.6875\n",
            "Epoch 63/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0951 - accuracy: 0.9847 - val_loss: 1.1645 - val_accuracy: 0.7188\n",
            "Epoch 64/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0895 - accuracy: 0.9847 - val_loss: 1.1938 - val_accuracy: 0.6562\n",
            "Epoch 65/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.1028 - accuracy: 0.9847 - val_loss: 1.1664 - val_accuracy: 0.7188\n",
            "Epoch 66/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0896 - accuracy: 0.9618 - val_loss: 1.2961 - val_accuracy: 0.7188\n",
            "Epoch 67/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.1077 - accuracy: 0.9771 - val_loss: 1.3944 - val_accuracy: 0.6562\n",
            "Epoch 68/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0780 - accuracy: 0.9771 - val_loss: 1.2528 - val_accuracy: 0.7188\n",
            "Epoch 69/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0586 - accuracy: 0.9924 - val_loss: 1.1092 - val_accuracy: 0.7500\n",
            "Epoch 70/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0508 - accuracy: 1.0000 - val_loss: 1.1662 - val_accuracy: 0.7188\n",
            "Epoch 71/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0517 - accuracy: 0.9924 - val_loss: 1.4787 - val_accuracy: 0.6875\n",
            "Epoch 72/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.1072 - accuracy: 0.9924 - val_loss: 1.2919 - val_accuracy: 0.7188\n",
            "Epoch 73/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0671 - accuracy: 0.9771 - val_loss: 1.3352 - val_accuracy: 0.6875\n",
            "Epoch 74/250\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0590 - accuracy: 0.9847 - val_loss: 1.3515 - val_accuracy: 0.6875\n",
            "Epoch 75/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0250 - accuracy: 1.0000 - val_loss: 1.3866 - val_accuracy: 0.7188\n",
            "Epoch 76/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0272 - accuracy: 0.9924 - val_loss: 1.5432 - val_accuracy: 0.6875\n",
            "Epoch 77/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0308 - accuracy: 0.9924 - val_loss: 1.4561 - val_accuracy: 0.7188\n",
            "Epoch 78/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0194 - accuracy: 1.0000 - val_loss: 1.3909 - val_accuracy: 0.7188\n",
            "Epoch 79/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0236 - accuracy: 0.9924 - val_loss: 1.4505 - val_accuracy: 0.7188\n",
            "Epoch 80/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0329 - accuracy: 1.0000 - val_loss: 1.4625 - val_accuracy: 0.7188\n",
            "Epoch 81/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0401 - accuracy: 0.9924 - val_loss: 1.4550 - val_accuracy: 0.7188\n",
            "Epoch 82/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 1.4970 - val_accuracy: 0.7188\n",
            "Epoch 83/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0319 - accuracy: 0.9924 - val_loss: 1.4868 - val_accuracy: 0.6875\n",
            "Epoch 84/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0384 - accuracy: 0.9924 - val_loss: 1.3442 - val_accuracy: 0.6875\n",
            "Epoch 85/250\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 1.3914 - val_accuracy: 0.6875\n",
            "Epoch 86/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0227 - accuracy: 0.9924 - val_loss: 1.4992 - val_accuracy: 0.7188\n",
            "Epoch 87/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 1.5867 - val_accuracy: 0.7188\n",
            "Epoch 88/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 1.6302 - val_accuracy: 0.7188\n",
            "Epoch 89/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 1.7097 - val_accuracy: 0.7188\n",
            "Epoch 90/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 1.7152 - val_accuracy: 0.7188\n",
            "Epoch 91/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 1.7268 - val_accuracy: 0.7188\n",
            "Epoch 92/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.7985 - val_accuracy: 0.7188\n",
            "Epoch 93/250\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.8689 - val_accuracy: 0.7188\n",
            "Epoch 94/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.8559 - val_accuracy: 0.7188\n",
            "Epoch 95/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.8205 - val_accuracy: 0.7188\n",
            "Epoch 96/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 2.1011 - val_accuracy: 0.6562\n",
            "Epoch 97/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.1771 - accuracy: 0.9542 - val_loss: 1.3338 - val_accuracy: 0.6875\n",
            "Epoch 98/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.1380 - accuracy: 0.9618 - val_loss: 1.0479 - val_accuracy: 0.6875\n",
            "Epoch 99/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0793 - accuracy: 0.9847 - val_loss: 1.0819 - val_accuracy: 0.7188\n",
            "Epoch 100/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0459 - accuracy: 0.9847 - val_loss: 1.3323 - val_accuracy: 0.6875\n",
            "Epoch 101/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0716 - accuracy: 0.9771 - val_loss: 1.3485 - val_accuracy: 0.6875\n",
            "Epoch 102/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0794 - accuracy: 0.9847 - val_loss: 1.1485 - val_accuracy: 0.7188\n",
            "Epoch 103/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0486 - accuracy: 1.0000 - val_loss: 0.9481 - val_accuracy: 0.6875\n",
            "Epoch 104/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0657 - accuracy: 0.9847 - val_loss: 0.9888 - val_accuracy: 0.7188\n",
            "Epoch 105/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0411 - accuracy: 0.9924 - val_loss: 1.2234 - val_accuracy: 0.7188\n",
            "Epoch 106/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 1.5106 - val_accuracy: 0.6875\n",
            "Epoch 107/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.8123 - val_accuracy: 0.6875\n",
            "Epoch 108/250\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 1.8995 - val_accuracy: 0.6875\n",
            "Epoch 109/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.8538 - val_accuracy: 0.6875\n",
            "Epoch 110/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.8370 - val_accuracy: 0.6875\n",
            "Epoch 111/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.8460 - val_accuracy: 0.6875\n",
            "Epoch 112/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.8586 - val_accuracy: 0.6875\n",
            "Epoch 113/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.8783 - val_accuracy: 0.6875\n",
            "Epoch 114/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.9067 - val_accuracy: 0.6875\n",
            "Epoch 115/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.9318 - val_accuracy: 0.6875\n",
            "Epoch 116/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.9497 - val_accuracy: 0.6875\n",
            "Epoch 117/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.9666 - val_accuracy: 0.6875\n",
            "Epoch 118/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.9730 - val_accuracy: 0.6875\n",
            "Epoch 119/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.9713 - val_accuracy: 0.6875\n",
            "Epoch 120/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.9684 - val_accuracy: 0.6875\n",
            "Epoch 121/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.9716 - val_accuracy: 0.6875\n",
            "Epoch 122/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.9925 - val_accuracy: 0.6875\n",
            "Epoch 123/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.0121 - val_accuracy: 0.6875\n",
            "Epoch 124/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.0165 - val_accuracy: 0.6875\n",
            "Epoch 125/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.0194 - val_accuracy: 0.6875\n",
            "Epoch 126/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.0306 - val_accuracy: 0.6875\n",
            "Epoch 127/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.0460 - val_accuracy: 0.6875\n",
            "Epoch 128/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.0645 - val_accuracy: 0.6875\n",
            "Epoch 129/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.0649 - val_accuracy: 0.6875\n",
            "Epoch 130/250\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.0696 - val_accuracy: 0.6875\n",
            "Epoch 131/250\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.0683 - val_accuracy: 0.6875\n",
            "Epoch 132/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.0757 - val_accuracy: 0.6875\n",
            "Epoch 133/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.0660 - val_accuracy: 0.6875\n",
            "Epoch 134/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.0697 - val_accuracy: 0.6875\n",
            "Epoch 135/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.0726 - val_accuracy: 0.6875\n",
            "Epoch 136/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 7.8926e-04 - accuracy: 1.0000 - val_loss: 2.0815 - val_accuracy: 0.6875\n",
            "Epoch 137/250\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 9.5062e-04 - accuracy: 1.0000 - val_loss: 2.0935 - val_accuracy: 0.6875\n",
            "Epoch 138/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 9.5103e-04 - accuracy: 1.0000 - val_loss: 2.1135 - val_accuracy: 0.6875\n",
            "Epoch 139/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.1394 - val_accuracy: 0.6875\n",
            "Epoch 140/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 7.0834e-04 - accuracy: 1.0000 - val_loss: 2.1549 - val_accuracy: 0.6875\n",
            "Epoch 141/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 7.0227e-04 - accuracy: 1.0000 - val_loss: 2.1711 - val_accuracy: 0.6875\n",
            "Epoch 142/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 9.2600e-04 - accuracy: 1.0000 - val_loss: 2.1855 - val_accuracy: 0.6875\n",
            "Epoch 143/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 5.9999e-04 - accuracy: 1.0000 - val_loss: 2.1987 - val_accuracy: 0.6875\n",
            "Epoch 144/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 7.0977e-04 - accuracy: 1.0000 - val_loss: 2.2094 - val_accuracy: 0.6875\n",
            "Epoch 145/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.2132 - val_accuracy: 0.6875\n",
            "Epoch 146/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 9.4288e-04 - accuracy: 1.0000 - val_loss: 2.2228 - val_accuracy: 0.6875\n",
            "Epoch 147/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.2321 - val_accuracy: 0.6875\n",
            "Epoch 148/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.2478 - val_accuracy: 0.6875\n",
            "Epoch 149/250\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.2640 - val_accuracy: 0.6875\n",
            "Epoch 150/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 8.7351e-04 - accuracy: 1.0000 - val_loss: 2.2634 - val_accuracy: 0.6875\n",
            "Epoch 151/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.1761 - val_accuracy: 0.6875\n",
            "Epoch 152/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.0598 - val_accuracy: 0.6875\n",
            "Epoch 153/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.0177 - val_accuracy: 0.6875\n",
            "Epoch 154/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.0322 - val_accuracy: 0.6875\n",
            "Epoch 155/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0077 - accuracy: 0.9924 - val_loss: 2.1255 - val_accuracy: 0.6250\n",
            "Epoch 156/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.1073 - accuracy: 0.9924 - val_loss: 1.6244 - val_accuracy: 0.6875\n",
            "Epoch 157/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0238 - accuracy: 0.9924 - val_loss: 1.5163 - val_accuracy: 0.6875\n",
            "Epoch 158/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0245 - accuracy: 0.9924 - val_loss: 1.2761 - val_accuracy: 0.7188\n",
            "Epoch 159/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0390 - accuracy: 0.9847 - val_loss: 1.3603 - val_accuracy: 0.7188\n",
            "Epoch 160/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.5610 - val_accuracy: 0.6875\n",
            "Epoch 161/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 1.6921 - val_accuracy: 0.6562\n",
            "Epoch 162/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0472 - accuracy: 0.9924 - val_loss: 1.6336 - val_accuracy: 0.6562\n",
            "Epoch 163/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0199 - accuracy: 0.9924 - val_loss: 1.8095 - val_accuracy: 0.6250\n",
            "Epoch 164/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 1.5353 - val_accuracy: 0.6562\n",
            "Epoch 165/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 1.5909 - val_accuracy: 0.7188\n",
            "Epoch 166/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.6812 - val_accuracy: 0.6562\n",
            "Epoch 167/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.7889 - val_accuracy: 0.6875\n",
            "Epoch 168/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.9503 - val_accuracy: 0.6875\n",
            "Epoch 169/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.1104 - val_accuracy: 0.6875\n",
            "Epoch 170/250\n",
            "5/5 [==============================] - 0s 29ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 2.1025 - val_accuracy: 0.6875\n",
            "Epoch 171/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.0140 - val_accuracy: 0.6875\n",
            "Epoch 172/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 8.7910e-04 - accuracy: 1.0000 - val_loss: 1.9718 - val_accuracy: 0.6875\n",
            "Epoch 173/250\n",
            "5/5 [==============================] - 0s 26ms/step - loss: 9.0010e-04 - accuracy: 1.0000 - val_loss: 1.9624 - val_accuracy: 0.6875\n",
            "Epoch 174/250\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.9627 - val_accuracy: 0.6875\n",
            "Epoch 175/250\n",
            "1/5 [=====>........................] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-191c31ffbc3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#model.save('model.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1387\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m     \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \"\"\"\n\u001b[1;32m   1222\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = pd.DataFrame(model.history.history)\n",
        "\n",
        "#plotting the loss and accuracy \n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "plt.plot(loss[\"loss\"], label =\"Loss\")\n",
        "plt.plot(loss[\"val_loss\"], label = \"Validation_loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.plot(loss['accuracy'],label = \"Training Accuracy\")\n",
        "plt.plot(loss['val_accuracy'], label =\"Validation_ Accuracy \")\n",
        "plt.legend()\n",
        "plt.title(\"Training-Validation Accuracy\")"
      ],
      "metadata": {
        "id": "JvWzB_usAAvy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}